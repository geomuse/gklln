---
layout: post
title:  python贪吃蛇游戏项目-强化学习介绍-01
date:   2025-08-08 09:01:00 +0800
tags: 
    - python
    - game
image: 03.jpg
---

### 认识强化学习 

教机器“边玩边学”，靠奖励信号来学会更聪明的决策方式

<!-- 如何创建价格函数就是一个目标

https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0

把其中观念整理好?

https://datawhalechina.github.io/easy-rl/#/chapter1/chapter1

看两本书整理重点. -->

智能体在不确定环境里得到最大的奖励

环境和智能体在互相交互中的动作提供当前状态的奖励

训练一个"价值估计函数"来采取动作

### 强化学习的困难

1. 我们可以发现智能体得到的观测（observation）不是独立同分布的，上一帧与下一帧间其实有非常强的连续性

我们得到的数据是相关的时间序列数据，不满足独立同分布。另外，我们并没有立刻获得反馈，游戏没有告诉我们哪个动作是正确动作

2. 因为智能体不能得到即时的反馈，然而我们依然希望智能体在这个环境中学习

这里我们就面临延迟奖励（delayed reward）的问题，延迟奖励使得训练网络非常困难

### 训练困难的地方

得到的是有时间关联的数据（sequential data）， 而不是独立同分布的数据。在机器学习中，如果观测数据有非常强的关联，会使得训练非常不稳定。这也是为什么在监督学习中，我们希望数据尽量满足独立同分布，这样就可以消除数据之间的相关性。