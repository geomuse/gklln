---
layout: post
title:  python网络爬虫实战-requests-2
date:   2025-07-28 09:01:00 +0800
tags: 
    - python
    - web
image: 02.jpg
---

#### 发起 POST 请求（提交表单）

```py
payload = {'username': 'test', 'password': '123456'}
response = requests.post('https://httpbin.org/post', data=payload)
print(response.json())
```

#### 发送 `PUT` 请求通常用于**更新资源**。例如：更新数据库中某条记录的内容。

```python
import requests

url = 'https://httpbin.org/put'
data = {
    'id': '123',
    'name': 'Boon Hong'
}

response = requests.put(url, data=data)
print(response.status_code)
print(response.text)
```

#### 发送 JSON 数据

```python
json_data = {
    'id': '123',
    'status': 'active'
}

response = requests.put(url, json=json_data)
print(response.json())
```
    
#### 发送 `DELETE` 请求用于**删除资源**，比如删除一个用户、文章、数据等。

```python
url = 'https://httpbin.org/delete'
params = {'id': '123'}

response = requests.delete(url, params=params)
print(response.status_code)
print(response.json())
```

#### 通过`get`爬取网页

`re` 为简单快速学习，正则表达式用chatgpt处理就好，我们只需要搞懂基本的web爬取流程即可。

```py
import requests , re

r = requests.get('https://ssr1.scrape.center/',verify=False)
exit() if not r.status_code == requests.codes.ok else print('ok')
pattern = re.compile('<h2.*?>(.*?)</h2>',re.S)
title = re.findall(pattern,r.text)
print(title)
```

#### 添加请求头（headers）

某些网站会发现这不是一个由正常游览器发送的请求，于是可能返回异常结果，导致网页爬取失败，于是请求头就解决这个问题。

```python
headers = {
    'User-Agent': 'Mozilla/5.0',
}
response = requests.get('https://example.com', headers=headers)
```

#### 添加 URL 参数

如果要附加额外信息，利用params参数就能直接传递这种信息

```python
params = {'q': 'python', 'lang': 'en'}
response = requests.get('https://httpbin.org/get', params=params)
```
<!-- 
#### 发送 JSON 数据

```python
json_data = {'key': 'value'}
response = requests.post('https://httpbin.org/post', json=json_data)
``` -->

#### 上传文件

```python
files = {'file': open('test.txt', 'rb')}
response = requests.post('https://httpbin.org/post', files=files)
```

#### 处理超时和异常

```python
try:
    response = requests.get('https://example.com', timeout=5)
except requests.Timeout:
    print('请求超时')
except requests.RequestException as e:
    print(f'请求失败: {e}')
```

#### 使用会话（Session）保持登录状态

```python
session = requests.Session()
session.get('https://httpbin.org/cookies/set/sessioncookie/123456789')
r = session.get('https://httpbin.org/cookies')
print(r.text)
```

#### 代理设置

```python
proxies = {
    'http': 'http://127.0.0.1:8080',
    'https': 'https://127.0.0.1:8080',
}
response = requests.get('https://httpbin.org/get', proxies=proxies)
```

#### 自带身份验证

```py
import requests 
from requests.auth import HTTPBasicAuth

r = requests.get('https://ssr3.scrape.center/',verify=False,auth=HTTPBasicAuth('admin','admin'))

print(r.status_code)
```