---
layout: post
title:  python网络爬虫实战-requests
date:   2025-07-27 09:01:00 +0800
tags: 
    - python
    - web
image: 02.jpg
---

大家好，我是python网络爬虫这门课程的主要讲师geo x telos lab

### request 的使用方法

可用于发送各种类型的 HTTP 请求，如 GET、POST、PUT、DELETE 等

#### 发起 GET 请求

该网站会判断客户端是否为GET请求，如果是，那么它将会返回相对应的请求信息

```py
import requests

response = requests.get('https://httpbin.org/get')
print(response.status_code)      # 状态码
print(response.text)             # 返回的 HTML 内容
```

#### 状态码 status code

这是通过server端会回传的状态码

服务器对客户端请求的响应结果，用来说明服务器是否成功处理了请求，或者发生了什么错误。

| 状态码   | 含义               | 示例说明               |
| ----- | ---------------- | ------------------ |
| `200` | OK，请求成功          | 最常见。成功获取网页、API 数据等 |
| `201` | Created，资源已创建    | 如 POST 创建资源成功      |
| `204` | No Content，无内容返回 | 通常用于 DELETE 操作     |
| `301` | 永久重定向            | 网址永久搬家     |
| `302` | 临时重定向            | 网址临时跳转     |
| `304` | Not Modified，未修改 | 浏览器缓存使用的响应 |
| `400` | Bad Request，请求错误       | 参数格式不对、缺字段   |
| `401` | Unauthorized，未授权       | 需要登录或 Token  |
| `403` | Forbidden，被禁止访问        | 有权限问题        |
| `404` | Not Found，资源未找到        | URL 错误或数据不存在 |
| `429` | Too Many Requests，过载请求 | 爬虫被封、限流触发    |
| `500` | Internal Server Error，服务器内部错误 | 程序崩溃或bug |
| `502` | Bad Gateway，网关错误              | 服务中间层出问题 |
| `503` | Service Unavailable，服务不可用     | 服务器过载或维护 |
| `504` | Gateway Timeout，网关超时          | 响应超时     |

#### 发起 POST 请求（提交表单）

```py
payload = {'username': 'test', 'password': '123456'}
response = requests.post('https://httpbin.org/post', data=payload)
print(response.json())
```

#### 发送 `PUT` 请求通常用于**更新资源**。例如：更新数据库中某条记录的内容。

```python
import requests

url = 'https://httpbin.org/put'
data = {
    'id': '123',
    'name': 'Boon Hong'
}

response = requests.put(url, data=data)
print(response.status_code)
print(response.text)
```

#### 发送 JSON 数据

```python
json_data = {
    'id': '123',
    'status': 'active'
}

response = requests.put(url, json=json_data)
print(response.json())
```

#### 发送 `DELETE` 请求用于**删除资源**，比如删除一个用户、文章、数据等。

```python
url = 'https://httpbin.org/delete'
params = {'id': '123'}

response = requests.delete(url, params=params)
print(response.status_code)
print(response.json())
```

#### 通过`get`爬取网页

`re` 为简单快速学习，正则表达式用chatgpt处理就好，我们只需要搞懂基本的web爬取流程即可。

```py
import requests , re

r = requests.get('https://ssr1.scrape.center/',verify=False)
exit() if not r.status_code == requests.codes.ok else print('ok')
pattern = re.compile('<h2.*?>(.*?)</h2>',re.S)
title = re.findall(pattern,r.text)
print(title)
```

#### 添加请求头（headers）

某些网站会发现这不是一个由正常游览器发送的请求，于是可能返回异常结果，导致网页爬取失败，于是请求头就解决这个问题。

```python
headers = {
    'User-Agent': 'Mozilla/5.0',
}
response = requests.get('https://example.com', headers=headers)
```

#### 添加 URL 参数

如果要附加额外信息，利用params参数就能直接传递这种信息

```python
params = {'q': 'python', 'lang': 'en'}
response = requests.get('https://httpbin.org/get', params=params)
```
<!-- 
#### 发送 JSON 数据

```python
json_data = {'key': 'value'}
response = requests.post('https://httpbin.org/post', json=json_data)
``` -->

#### 上传文件

```python
files = {'file': open('test.txt', 'rb')}
response = requests.post('https://httpbin.org/post', files=files)
```

#### 处理超时和异常

```python
try:
    response = requests.get('https://example.com', timeout=5)
except requests.Timeout:
    print('请求超时')
except requests.RequestException as e:
    print(f'请求失败: {e}')
```

---

#### 使用会话（Session）保持登录状态

```python
session = requests.Session()
session.get('https://httpbin.org/cookies/set/sessioncookie/123456789')
r = session.get('https://httpbin.org/cookies')
print(r.text)
```

#### 代理设置

```python
proxies = {
    'http': 'http://127.0.0.1:8080',
    'https': 'https://127.0.0.1:8080',
}
response = requests.get('https://httpbin.org/get', proxies=proxies)
```

#### 自带身份验证

```py
import requests 
from requests.auth import HTTPBasicAuth

r = requests.get('https://ssr3.scrape.center/',verify=False,auth=HTTPBasicAuth('admin','admin'))

print(r.status_code)
```