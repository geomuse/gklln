---
layout: post
title:  python网络爬虫实战-requests
date:   2025-07-27 09:01:00 +0800
tags: 
    - python
    - web
image: 02.jpg
---

大家好，我是python网络爬虫这门课程的主要讲师geo 

# 什么是requests

Python 中一个 非常简单易用的 HTTP 请求库，主要用来与网络上的服务器进行沟通

# requests 的使用方法

可用于发送各种类型的 HTTP 请求，如 GET、POST、PUT、DELETE 等

## 发起 GET 请求

该网站会判断客户端是否为GET请求，如果是，那么它将会返回相对应的请求信息

```py
import requests

response = requests.get('https://httpbin.org/get')
print(response.status_code)      # 状态码
print(response.text)             # 返回的 HTML 内容
```

## 状态码 status code

这是通过server端会回传的状态码

服务器对客户端请求的响应结果，用来说明服务器是否成功处理了请求，或者发生了什么错误。

| 状态码   | 含义               | 示例说明               |
| ----- | ---------------- | ------------------ |
| `200` | OK，请求成功          | 最常见。成功获取网页、API 数据等 |
| `201` | Created，资源已创建    | 如 POST 创建资源成功      |
| `204` | No Content，无内容返回 | 通常用于 DELETE 操作     |
| `301` | 永久重定向            | 网址永久搬家     |
| `302` | 临时重定向            | 网址临时跳转     |
| `304` | Not Modified，未修改 | 浏览器缓存使用的响应 |
| `400` | Bad Request，请求错误       | 参数格式不对、缺字段   |
| `401` | Unauthorized，未授权       | 需要登录或 Token  |
| `403` | Forbidden，被禁止访问        | 有权限问题        |
| `404` | Not Found，资源未找到        | URL 错误或数据不存在 |
| `429` | Too Many Requests，过载请求 | 爬虫被封、限流触发    |
| `500` | Internal Server Error，服务器内部错误 | 程序崩溃或bug |
| `502` | Bad Gateway，网关错误              | 服务中间层出问题 |
| `503` | Service Unavailable，服务不可用     | 服务器过载或维护 |
| `504` | Gateway Timeout，网关超时          | 响应超时     |

对url进行get请求

# 发起 POST 请求（提交表单）

```py
payload = {'username': 'test', 'password': '123456'}
response = requests.post('https://httpbin.org/post', data=payload)
print(response.json())
```

# 发送 `PUT` 请求通常用于**更新资源**。例如：更新数据库中某条记录的内容。

```python
import requests

url = 'https://httpbin.org/put'
data = {
    'id': '123',
    'name': 'Boon Hong'
}

response = requests.put(url, data=data)
print(response.status_code)
print(response.text)
```

# 发送 JSON 数据

```python
json_data = {
    'id': '123',
    'status': 'active'
}

response = requests.put(url, json=json_data)
print(response.json())
```
    
#### 发送 `DELETE` 请求用于**删除资源**，比如删除一个用户、文章、数据等。

```python
url = 'https://httpbin.org/delete'
params = {'id': '123'}

response = requests.delete(url, params=params)
print(response.status_code)
print(response.json())
```

<!-- # 通过`get`爬取网页

`re` 为简单快速学习，正则表达式用chatgpt处理就好，我们只需要搞懂基本的web爬取流程即可。

```py
import requests , re

r = requests.get('https://ssr1.scrape.center/',verify=False)
exit() if not r.status_code == requests.codes.ok else print('ok')
pattern = re.compile('<h2.*?>(.*?)</h2>',re.S)
title = re.findall(pattern,r.text)
print(title)
``` -->