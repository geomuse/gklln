---
layout: post
title:  python网络爬虫实战-selenium
date:   2025-08-01 09:01:00 +0800
tags: 
    - python
    - web
image: 05.jpg
---

大家好，我是python网络爬虫这门课程的主要讲师geo

Selenium 是一个开源的 Web 自动化测试框架，主要用于模拟用户在浏览器中的操作，比如点击、输入、页面跳转、抓取数据等。
它最初是为 Web 应用自动化测试 设计的，但现在也常用于 爬虫 和 自动化任务。

动态网页的内容是由 JavaScript 在页面加载后动态生成的，常规的 requests 抓不到这些数据。这时我们就需要 Selenium，它能模拟用户操作浏览器，加载完整页面后再提取内容。

```bash
pip install selenium
```

下载 GeckoDriver（Firefox 的驱动）

GeckoDriver 的 GitHub 发布页面

https://github.com/mozilla/geckodriver/releases

演示一个 Selenium 打开网页并搜索的例子

```py
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

# 启动 Chrome 浏览器
driver = webdriver.Chrome()

# 打开 Google
driver.get("https://www.google.com")

# 找到搜索框并输入内容
search_box = driver.find_element(By.NAME, "q")
search_box.send_keys("Selenium Python")
search_box.send_keys(Keys.RETURN)

# 输出当前网页标题
print(driver.title)

# 关闭浏览器
driver.quit()
```

### 指定geckodriver路径

```py
from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.by import By
import time

# 指定 geckodriver 路径
service = Service(executable_path='geckodriver路径')

# 初始化 Firefox 浏览器
firefox = webdriver.Firefox(service=service)

# 打开动态网页
firefox.get("https://example.com")

# 等待页面动态加载
time.sleep(5)

# 抓取网页元素
element = firefox.find_element(By.XPATH, "//h1")
print("抓取的内容:", element.text)

# 关闭浏览器
firefox.close()
```

### 无头模式（Headless Firefox）

无头模式可以后台运行，不弹出浏览器窗口

```py
from selenium.webdriver.firefox.options import Options

options = Options()
options.add_argument("--headless")  # 开启无头模式
driver = webdriver.Firefox(service=service, options=options)
```

### 请求等待

```py
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

wait = WebDriverWait(driver, 10)
element = wait.until(EC.presence_of_element_located((By.CLASS_NAME, "product-title")))
print(element.text)
```

<!-- ### 向下滚动页面（加载更多内容）

```py
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height
``` -->

<!-- ### 结合 BeautifulSoup 分析页面

```py
from bs4 import BeautifulSoup

html = driver.page_source
soup = BeautifulSoup(html, "html.parser")

for item in soup.select(".product-title"):
    print(item.text)
```

### CSV 数据保存

```py
import csv

driver.get("https://example.com/products")
time.sleep(3)

# 获取多个商品
products = driver.find_elements(By.CLASS_NAME, "product-title")
for product in products:
    print(product.text)

with open("data.csv", "w", newline='', encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["标题"])
    for p in products:
        writer.writerow([p.text])
``` -->